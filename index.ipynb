{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets\n",
    "if not os.path.exists('sales_textbook.txt'):\n",
    "  url = 'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/resolve/main/sales_textbook.txt?download=true'\n",
    "  with open('sales_textbook.txt','wb') as f:\n",
    "    f.write(requests.get(url).content)\n",
    "\n",
    "# read content to memory\n",
    "with open('sales_textbook.txt','r') as f:\n",
    "  text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77919"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize origin datasets\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "tokenized_text = encoding.encode(text)\n",
    "# list to tensor\n",
    "tokenized_text = torch.tensor(tokenized_text,dtype=torch.long)\n",
    "max_token_value = tokenized_text.max().item()\n",
    "len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26072,   220,    16,  ...,  1501, 48451,  7119])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train sets and validate sets\n",
    "train_idx = int(len(tokenized_text) * 0.9)\n",
    "train_data = tokenized_text[:train_idx]\n",
    "valid_data = tokenized_text[train_idx:]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "context_length = 16 # split input ant the input include 16 token\n",
    "d_model = 64\n",
    "batch_size = 4 # 4 train parallel\n",
    "num_heads = 4 # multi head num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4443, 94066,   430, 11415,   701,  1866, 11704,   323,  1268,   814,\n",
       "         29243,   311,   279,  6130,   596,  6671],\n",
       "        [  315,   701, 10209,    11,   499,   649, 61705, 40017,   323,  1798,\n",
       "           484, 12410,   304,   701,  4754,  6444],\n",
       "        [ 3339,   264,  4062,  6412,   627,    19,    13, 67118, 36755,   323,\n",
       "         61913,    25,  2057,  4726, 18885, 38769],\n",
       "        [  892,    11,  5376, 26206,    11,  8108,  7194,    11,   477,  6493,\n",
       "           904,  1023,  5199, 20124,   430,  5398]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train datastruct should be [4,16,64] 4 batch 16 token 64 dimension\n",
    "data = train_data\n",
    "idxs = torch.randint(low=0,high=len(data)-context_length,size=(batch_size,)) # rand 4 batch start index position\n",
    "x_batch = torch.stack([data[idx:idx+context_length] for idx in idxs])\n",
    "y_batch = torch.stack([data[idx+1:idx+context_length+1] for idx in idxs])\n",
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 64])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input embedding table\n",
    "# row express One of 16 tokens, column express 64 dimension\n",
    "# step 1 : create a full table row of all token, and column is 64 dimension\n",
    "input_embedding_lookup_table = nn.Embedding(max_token_value + 1,d_model)\n",
    "\n",
    "x_batch_embedding = input_embedding_lookup_table(x_batch)\n",
    "y_batch_embedding = input_embedding_lookup_table(y_batch)\n",
    "x_batch_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding sin cos TODO: here idk so copy paper\n",
    "position_encoding_lookup_table = torch.zeros(context_length,d_model)\n",
    "position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "# apply the sine & cosine\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)\n",
    "position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)\n",
    "position_encoding_lookup_table = position_encoding_lookup_table.unsqueeze(0).expand(batch_size, -1, -1) # add batch to the first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.322218</td>\n",
       "      <td>-0.409389</td>\n",
       "      <td>2.188540</td>\n",
       "      <td>0.902386</td>\n",
       "      <td>0.842244</td>\n",
       "      <td>1.357249</td>\n",
       "      <td>0.376062</td>\n",
       "      <td>1.452828</td>\n",
       "      <td>-0.147984</td>\n",
       "      <td>1.993450</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.211121</td>\n",
       "      <td>0.208891</td>\n",
       "      <td>1.960570</td>\n",
       "      <td>0.402269</td>\n",
       "      <td>-0.091994</td>\n",
       "      <td>0.989842</td>\n",
       "      <td>0.460002</td>\n",
       "      <td>1.804185</td>\n",
       "      <td>-0.810581</td>\n",
       "      <td>1.048166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.672771</td>\n",
       "      <td>0.354527</td>\n",
       "      <td>1.620251</td>\n",
       "      <td>-0.174871</td>\n",
       "      <td>1.348436</td>\n",
       "      <td>0.234261</td>\n",
       "      <td>0.614191</td>\n",
       "      <td>2.486705</td>\n",
       "      <td>-0.349069</td>\n",
       "      <td>1.929204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.773519</td>\n",
       "      <td>0.294397</td>\n",
       "      <td>-0.880421</td>\n",
       "      <td>0.194865</td>\n",
       "      <td>-0.433929</td>\n",
       "      <td>1.150474</td>\n",
       "      <td>0.672945</td>\n",
       "      <td>0.023306</td>\n",
       "      <td>-0.934006</td>\n",
       "      <td>2.711809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.908097</td>\n",
       "      <td>-1.060022</td>\n",
       "      <td>1.584182</td>\n",
       "      <td>0.579031</td>\n",
       "      <td>1.203130</td>\n",
       "      <td>1.127968</td>\n",
       "      <td>1.182322</td>\n",
       "      <td>1.610960</td>\n",
       "      <td>0.619247</td>\n",
       "      <td>2.679089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450202</td>\n",
       "      <td>-0.293924</td>\n",
       "      <td>-1.256550</td>\n",
       "      <td>2.188431</td>\n",
       "      <td>-0.790716</td>\n",
       "      <td>-1.366608</td>\n",
       "      <td>-0.111572</td>\n",
       "      <td>1.116148</td>\n",
       "      <td>-0.669431</td>\n",
       "      <td>1.716500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.487071</td>\n",
       "      <td>0.524812</td>\n",
       "      <td>0.870774</td>\n",
       "      <td>-0.572864</td>\n",
       "      <td>1.849003</td>\n",
       "      <td>0.477404</td>\n",
       "      <td>1.363102</td>\n",
       "      <td>0.873452</td>\n",
       "      <td>0.763274</td>\n",
       "      <td>0.746941</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143813</td>\n",
       "      <td>0.537079</td>\n",
       "      <td>1.759755</td>\n",
       "      <td>1.016758</td>\n",
       "      <td>0.141407</td>\n",
       "      <td>-0.381310</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>1.125835</td>\n",
       "      <td>-1.243398</td>\n",
       "      <td>0.474337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.519860</td>\n",
       "      <td>0.411349</td>\n",
       "      <td>1.014437</td>\n",
       "      <td>-1.386681</td>\n",
       "      <td>1.210713</td>\n",
       "      <td>0.377782</td>\n",
       "      <td>0.134280</td>\n",
       "      <td>-0.459338</td>\n",
       "      <td>1.058506</td>\n",
       "      <td>-0.503252</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.076302</td>\n",
       "      <td>1.209980</td>\n",
       "      <td>0.884415</td>\n",
       "      <td>1.615863</td>\n",
       "      <td>-2.223127</td>\n",
       "      <td>2.164112</td>\n",
       "      <td>-0.214202</td>\n",
       "      <td>1.881279</td>\n",
       "      <td>-0.946214</td>\n",
       "      <td>1.283735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.396897</td>\n",
       "      <td>1.098407</td>\n",
       "      <td>-0.731618</td>\n",
       "      <td>-0.477528</td>\n",
       "      <td>-0.815435</td>\n",
       "      <td>-1.185984</td>\n",
       "      <td>1.537297</td>\n",
       "      <td>-0.309508</td>\n",
       "      <td>2.694764</td>\n",
       "      <td>-0.646601</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.054133</td>\n",
       "      <td>0.790007</td>\n",
       "      <td>0.250778</td>\n",
       "      <td>2.261299</td>\n",
       "      <td>0.654142</td>\n",
       "      <td>2.518198</td>\n",
       "      <td>0.024840</td>\n",
       "      <td>1.194373</td>\n",
       "      <td>0.514472</td>\n",
       "      <td>1.918137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.392771</td>\n",
       "      <td>-0.373341</td>\n",
       "      <td>-0.251940</td>\n",
       "      <td>-0.014452</td>\n",
       "      <td>0.443205</td>\n",
       "      <td>-0.281049</td>\n",
       "      <td>0.683129</td>\n",
       "      <td>-1.289763</td>\n",
       "      <td>0.071701</td>\n",
       "      <td>0.881678</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.230057</td>\n",
       "      <td>0.363818</td>\n",
       "      <td>-0.743048</td>\n",
       "      <td>1.470696</td>\n",
       "      <td>-0.453912</td>\n",
       "      <td>1.795517</td>\n",
       "      <td>1.661132</td>\n",
       "      <td>-0.178232</td>\n",
       "      <td>-1.318180</td>\n",
       "      <td>1.084545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.437740</td>\n",
       "      <td>0.037849</td>\n",
       "      <td>-1.743038</td>\n",
       "      <td>0.640131</td>\n",
       "      <td>1.359085</td>\n",
       "      <td>0.614568</td>\n",
       "      <td>1.659745</td>\n",
       "      <td>1.367679</td>\n",
       "      <td>0.181814</td>\n",
       "      <td>1.297775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359023</td>\n",
       "      <td>-0.538674</td>\n",
       "      <td>-0.981430</td>\n",
       "      <td>2.012137</td>\n",
       "      <td>-1.642203</td>\n",
       "      <td>0.385722</td>\n",
       "      <td>1.761279</td>\n",
       "      <td>1.354689</td>\n",
       "      <td>-0.668779</td>\n",
       "      <td>1.353627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.984924</td>\n",
       "      <td>0.349465</td>\n",
       "      <td>-1.364149</td>\n",
       "      <td>0.147823</td>\n",
       "      <td>-0.334512</td>\n",
       "      <td>-0.996610</td>\n",
       "      <td>-1.080562</td>\n",
       "      <td>-0.989027</td>\n",
       "      <td>-0.609670</td>\n",
       "      <td>-0.733291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704268</td>\n",
       "      <td>0.727815</td>\n",
       "      <td>-2.259962</td>\n",
       "      <td>-0.146118</td>\n",
       "      <td>-0.091895</td>\n",
       "      <td>1.799253</td>\n",
       "      <td>0.521220</td>\n",
       "      <td>2.500714</td>\n",
       "      <td>-1.537430</td>\n",
       "      <td>0.976813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.509491</td>\n",
       "      <td>-0.941041</td>\n",
       "      <td>1.538565</td>\n",
       "      <td>0.585983</td>\n",
       "      <td>-0.444309</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>1.327218</td>\n",
       "      <td>-1.755941</td>\n",
       "      <td>0.969801</td>\n",
       "      <td>-1.157845</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.320791</td>\n",
       "      <td>2.203557</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>1.589249</td>\n",
       "      <td>-1.114987</td>\n",
       "      <td>1.971091</td>\n",
       "      <td>-0.618653</td>\n",
       "      <td>1.556947</td>\n",
       "      <td>-0.296786</td>\n",
       "      <td>2.578598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.100901</td>\n",
       "      <td>-1.288161</td>\n",
       "      <td>2.003289</td>\n",
       "      <td>0.853602</td>\n",
       "      <td>-1.006686</td>\n",
       "      <td>0.489731</td>\n",
       "      <td>-0.888482</td>\n",
       "      <td>0.675430</td>\n",
       "      <td>-0.641816</td>\n",
       "      <td>-0.913718</td>\n",
       "      <td>...</td>\n",
       "      <td>2.334639</td>\n",
       "      <td>1.872415</td>\n",
       "      <td>-0.456970</td>\n",
       "      <td>0.679741</td>\n",
       "      <td>-1.548463</td>\n",
       "      <td>1.582327</td>\n",
       "      <td>0.903912</td>\n",
       "      <td>0.861012</td>\n",
       "      <td>-1.042751</td>\n",
       "      <td>2.029799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.216246</td>\n",
       "      <td>-0.307929</td>\n",
       "      <td>1.539264</td>\n",
       "      <td>0.729747</td>\n",
       "      <td>-0.302923</td>\n",
       "      <td>-1.187275</td>\n",
       "      <td>-1.516870</td>\n",
       "      <td>0.331356</td>\n",
       "      <td>0.044383</td>\n",
       "      <td>-0.974086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.754749</td>\n",
       "      <td>0.685553</td>\n",
       "      <td>1.279468</td>\n",
       "      <td>0.546864</td>\n",
       "      <td>0.718338</td>\n",
       "      <td>1.363825</td>\n",
       "      <td>-1.053627</td>\n",
       "      <td>0.016471</td>\n",
       "      <td>-0.107710</td>\n",
       "      <td>1.490824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-2.562209</td>\n",
       "      <td>0.265924</td>\n",
       "      <td>0.888836</td>\n",
       "      <td>-1.100037</td>\n",
       "      <td>-0.158467</td>\n",
       "      <td>0.460589</td>\n",
       "      <td>-1.433979</td>\n",
       "      <td>-1.272111</td>\n",
       "      <td>0.301404</td>\n",
       "      <td>-0.070615</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.394767</td>\n",
       "      <td>0.768512</td>\n",
       "      <td>0.457245</td>\n",
       "      <td>0.471717</td>\n",
       "      <td>0.490315</td>\n",
       "      <td>1.279584</td>\n",
       "      <td>-0.009980</td>\n",
       "      <td>0.482476</td>\n",
       "      <td>-0.514546</td>\n",
       "      <td>-0.017058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.508628</td>\n",
       "      <td>0.689255</td>\n",
       "      <td>-0.411258</td>\n",
       "      <td>-0.965230</td>\n",
       "      <td>0.811605</td>\n",
       "      <td>0.843419</td>\n",
       "      <td>0.118066</td>\n",
       "      <td>1.117439</td>\n",
       "      <td>-1.198777</td>\n",
       "      <td>-0.060406</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.157478</td>\n",
       "      <td>1.981962</td>\n",
       "      <td>-0.232599</td>\n",
       "      <td>3.515503</td>\n",
       "      <td>-1.705072</td>\n",
       "      <td>1.721175</td>\n",
       "      <td>-0.054989</td>\n",
       "      <td>-0.467915</td>\n",
       "      <td>-0.496936</td>\n",
       "      <td>0.678522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.224246</td>\n",
       "      <td>1.000082</td>\n",
       "      <td>-0.342903</td>\n",
       "      <td>0.057385</td>\n",
       "      <td>2.536629</td>\n",
       "      <td>0.605027</td>\n",
       "      <td>-0.298996</td>\n",
       "      <td>0.849322</td>\n",
       "      <td>-2.496926</td>\n",
       "      <td>0.721138</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036060</td>\n",
       "      <td>2.839912</td>\n",
       "      <td>-1.057878</td>\n",
       "      <td>-0.786355</td>\n",
       "      <td>-0.531979</td>\n",
       "      <td>1.267633</td>\n",
       "      <td>-0.721827</td>\n",
       "      <td>-0.016903</td>\n",
       "      <td>1.808906</td>\n",
       "      <td>0.935801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.287855</td>\n",
       "      <td>-0.237307</td>\n",
       "      <td>-0.073482</td>\n",
       "      <td>1.299801</td>\n",
       "      <td>2.299878</td>\n",
       "      <td>-0.465521</td>\n",
       "      <td>-1.254613</td>\n",
       "      <td>1.923658</td>\n",
       "      <td>-2.449185</td>\n",
       "      <td>0.567873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116270</td>\n",
       "      <td>1.348915</td>\n",
       "      <td>0.359393</td>\n",
       "      <td>1.254575</td>\n",
       "      <td>-0.044456</td>\n",
       "      <td>0.294622</td>\n",
       "      <td>1.973787</td>\n",
       "      <td>0.078876</td>\n",
       "      <td>0.949411</td>\n",
       "      <td>1.088735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0  -0.322218 -0.409389  2.188540  0.902386  0.842244  1.357249  0.376062   \n",
       "1   1.672771  0.354527  1.620251 -0.174871  1.348436  0.234261  0.614191   \n",
       "2   0.908097 -1.060022  1.584182  0.579031  1.203130  1.127968  1.182322   \n",
       "3   0.487071  0.524812  0.870774 -0.572864  1.849003  0.477404  1.363102   \n",
       "4  -1.519860  0.411349  1.014437 -1.386681  1.210713  0.377782  0.134280   \n",
       "5  -0.396897  1.098407 -0.731618 -0.477528 -0.815435 -1.185984  1.537297   \n",
       "6  -0.392771 -0.373341 -0.251940 -0.014452  0.443205 -0.281049  0.683129   \n",
       "7   1.437740  0.037849 -1.743038  0.640131  1.359085  0.614568  1.659745   \n",
       "8   0.984924  0.349465 -1.364149  0.147823 -0.334512 -0.996610 -1.080562   \n",
       "9   0.509491 -0.941041  1.538565  0.585983 -0.444309  0.850889  1.327218   \n",
       "10  0.100901 -1.288161  2.003289  0.853602 -1.006686  0.489731 -0.888482   \n",
       "11 -1.216246 -0.307929  1.539264  0.729747 -0.302923 -1.187275 -1.516870   \n",
       "12 -2.562209  0.265924  0.888836 -1.100037 -0.158467  0.460589 -1.433979   \n",
       "13  0.508628  0.689255 -0.411258 -0.965230  0.811605  0.843419  0.118066   \n",
       "14 -1.224246  1.000082 -0.342903  0.057385  2.536629  0.605027 -0.298996   \n",
       "15  0.287855 -0.237307 -0.073482  1.299801  2.299878 -0.465521 -1.254613   \n",
       "\n",
       "          7         8         9   ...        54        55        56        57  \\\n",
       "0   1.452828 -0.147984  1.993450  ... -1.211121  0.208891  1.960570  0.402269   \n",
       "1   2.486705 -0.349069  1.929204  ... -0.773519  0.294397 -0.880421  0.194865   \n",
       "2   1.610960  0.619247  2.679089  ...  0.450202 -0.293924 -1.256550  2.188431   \n",
       "3   0.873452  0.763274  0.746941  ...  1.143813  0.537079  1.759755  1.016758   \n",
       "4  -0.459338  1.058506 -0.503252  ... -1.076302  1.209980  0.884415  1.615863   \n",
       "5  -0.309508  2.694764 -0.646601  ... -0.054133  0.790007  0.250778  2.261299   \n",
       "6  -1.289763  0.071701  0.881678  ... -1.230057  0.363818 -0.743048  1.470696   \n",
       "7   1.367679  0.181814  1.297775  ...  0.359023 -0.538674 -0.981430  2.012137   \n",
       "8  -0.989027 -0.609670 -0.733291  ...  0.704268  0.727815 -2.259962 -0.146118   \n",
       "9  -1.755941  0.969801 -1.157845  ... -1.320791  2.203557  0.003226  1.589249   \n",
       "10  0.675430 -0.641816 -0.913718  ...  2.334639  1.872415 -0.456970  0.679741   \n",
       "11  0.331356  0.044383 -0.974086  ... -0.754749  0.685553  1.279468  0.546864   \n",
       "12 -1.272111  0.301404 -0.070615  ... -1.394767  0.768512  0.457245  0.471717   \n",
       "13  1.117439 -1.198777 -0.060406  ... -1.157478  1.981962 -0.232599  3.515503   \n",
       "14  0.849322 -2.496926  0.721138  ...  1.036060  2.839912 -1.057878 -0.786355   \n",
       "15  1.923658 -2.449185  0.567873  ...  0.116270  1.348915  0.359393  1.254575   \n",
       "\n",
       "          58        59        60        61        62        63  \n",
       "0  -0.091994  0.989842  0.460002  1.804185 -0.810581  1.048166  \n",
       "1  -0.433929  1.150474  0.672945  0.023306 -0.934006  2.711809  \n",
       "2  -0.790716 -1.366608 -0.111572  1.116148 -0.669431  1.716500  \n",
       "3   0.141407 -0.381310  0.001009  1.125835 -1.243398  0.474337  \n",
       "4  -2.223127  2.164112 -0.214202  1.881279 -0.946214  1.283735  \n",
       "5   0.654142  2.518198  0.024840  1.194373  0.514472  1.918137  \n",
       "6  -0.453912  1.795517  1.661132 -0.178232 -1.318180  1.084545  \n",
       "7  -1.642203  0.385722  1.761279  1.354689 -0.668779  1.353627  \n",
       "8  -0.091895  1.799253  0.521220  2.500714 -1.537430  0.976813  \n",
       "9  -1.114987  1.971091 -0.618653  1.556947 -0.296786  2.578598  \n",
       "10 -1.548463  1.582327  0.903912  0.861012 -1.042751  2.029799  \n",
       "11  0.718338  1.363825 -1.053627  0.016471 -0.107710  1.490824  \n",
       "12  0.490315  1.279584 -0.009980  0.482476 -0.514546 -0.017058  \n",
       "13 -1.705072  1.721175 -0.054989 -0.467915 -0.496936  0.678522  \n",
       "14 -0.531979  1.267633 -0.721827 -0.016903  1.808906  0.935801  \n",
       "15 -0.044456  0.294622  1.973787  0.078876  0.949411  1.088735  \n",
       "\n",
       "[16 rows x 64 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add positional encoding to the input embedding\n",
    "x = x_batch_embedding + position_encoding_lookup_table\n",
    "y = y_batch_embedding + position_encoding_lookup_table\n",
    "pd.DataFrame(x[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 16, 64]), torch.Size([4, 16, 64]), torch.Size([4, 16, 64]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate Q,K,V\n",
    "Wq = nn.Linear(d_model,d_model)\n",
    "Wk = nn.Linear(d_model,d_model)\n",
    "Wv = nn.Linear(d_model,d_model)\n",
    "# default last [16 ,64] * [64,64] => [16,64], execute 4 batch auto\n",
    "Q = Wq(x)\n",
    "K = Wk(x)\n",
    "V = Wv(x)\n",
    "Q.shape,K.shape,V.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 16, 16])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply multi-head\n",
    "Q = Q.reshape(batch_size,context_length,num_heads,d_model//num_heads)\n",
    "K = K.reshape(batch_size,context_length,num_heads,d_model//num_heads)\n",
    "V = V.reshape(batch_size,context_length,num_heads,d_model//num_heads)\n",
    "\n",
    "Q = Q.permute(0,2,1,3) # [4,16,4,16] to [4,4,16,16]\n",
    "K = K.permute(0,2,1,3) # [4,16,4,16] to [4,4,16,16]\n",
    "V = V.permute(0,2,1,3) # [4,16,4,16] to [4,4,16,16]\n",
    "Q.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.1179e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-8.4912e-01, -2.9320e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-6.8314e-01, -6.0089e-01, -6.0636e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [-7.2487e-02, -3.3676e-01,  4.4331e-01,  ...,  9.9694e-01,\n",
       "                  -inf,        -inf],\n",
       "          [ 2.7911e-01, -2.3307e-01,  1.2442e+00,  ...,  1.1212e+00,\n",
       "            6.8239e-01,        -inf],\n",
       "          [-1.9782e-01, -8.2563e-02,  6.4668e-01,  ...,  1.0883e+00,\n",
       "            1.0151e+00,  7.0050e-01]],\n",
       "\n",
       "         [[ 5.3915e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-6.1488e-02, -3.4297e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 7.7564e-02,  1.1729e-01,  3.6037e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [-3.8849e-01,  2.0574e-01, -1.0657e-01,  ...,  5.0671e-01,\n",
       "                  -inf,        -inf],\n",
       "          [ 1.2254e+00, -2.3318e-01, -2.3906e-01,  ...,  1.9695e-02,\n",
       "           -4.0651e-01,        -inf],\n",
       "          [-4.1830e-01,  8.0246e-01,  3.3681e-01,  ..., -1.2640e-01,\n",
       "           -3.4434e-01, -3.6274e-01]],\n",
       "\n",
       "         [[-2.5353e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-2.2696e-01, -4.9735e-02,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-1.0072e-01,  1.4829e-01,  1.4348e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [ 2.7136e-01,  3.6369e-01,  1.1817e-01,  ..., -3.5815e-02,\n",
       "                  -inf,        -inf],\n",
       "          [ 2.6431e-02,  4.7735e-01, -2.8871e-01,  ...,  2.0115e-01,\n",
       "            4.6630e-01,        -inf],\n",
       "          [ 2.3142e-01,  2.4234e-02, -1.1341e-01,  ...,  2.3642e-01,\n",
       "           -3.3225e-01,  1.0961e+00]],\n",
       "\n",
       "         [[-1.3817e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 1.3946e-01,  3.1912e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-1.6973e-02, -4.1236e-02,  7.0665e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [ 1.6461e-01, -1.9252e-01, -2.4422e-01,  ..., -4.2853e-01,\n",
       "                  -inf,        -inf],\n",
       "          [-1.1619e-01, -1.4918e-01,  3.5243e-01,  ...,  7.6411e-01,\n",
       "            1.9701e-01,        -inf],\n",
       "          [ 3.5940e-01, -1.8963e-01, -2.1160e-02,  ...,  2.0539e-01,\n",
       "            1.1660e-01, -3.4566e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.0594e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-6.3418e-01, -2.5039e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-6.1842e-01, -5.9718e-01, -6.9712e-03,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [-1.4409e-03,  3.7538e-02,  4.0369e-02,  ...,  5.8004e-01,\n",
       "                  -inf,        -inf],\n",
       "          [-1.1960e-01, -3.8886e-01,  2.3055e-04,  ..., -1.5836e-01,\n",
       "           -7.8704e-02,        -inf],\n",
       "          [-2.2776e-01, -1.8970e-01,  4.5236e-02,  ...,  7.6806e-01,\n",
       "            3.6507e-01,  2.8457e-01]],\n",
       "\n",
       "         [[ 3.4121e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-2.1246e-02,  1.3202e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-7.9227e-01, -7.6951e-01, -2.6665e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [-1.6876e-02,  2.5734e-01,  1.1091e-01,  ...,  6.2434e-01,\n",
       "                  -inf,        -inf],\n",
       "          [-9.1736e-01, -8.8525e-01, -5.8212e-01,  ..., -1.5781e-01,\n",
       "           -4.8532e-01,        -inf],\n",
       "          [-1.0317e+00, -8.1601e-01, -1.2295e+00,  ...,  1.3978e-01,\n",
       "           -1.0129e+00, -3.3932e-01]],\n",
       "\n",
       "         [[-9.8123e-02,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 2.3978e-01,  2.3828e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-1.9645e-01,  3.4059e-01,  7.9887e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [ 1.0739e+00,  4.1442e-01,  5.6117e-02,  ...,  7.1613e-01,\n",
       "                  -inf,        -inf],\n",
       "          [ 1.9746e-01,  5.1891e-01,  1.0957e+00,  ..., -3.7369e-01,\n",
       "            1.1334e+00,        -inf],\n",
       "          [ 1.4734e+00,  6.3914e-01,  7.6025e-01,  ...,  1.4289e-01,\n",
       "            7.5428e-01,  2.8706e-01]],\n",
       "\n",
       "         [[ 2.0969e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 3.0549e-01,  1.1357e+00,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-8.0090e-02,  2.8122e-01,  2.1261e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [-2.0954e-01,  3.4737e-01, -1.1910e-01,  ...,  9.4376e-01,\n",
       "                  -inf,        -inf],\n",
       "          [-5.2045e-02, -1.6521e-03, -7.7824e-02,  ...,  2.0153e-01,\n",
       "           -6.5605e-02,        -inf],\n",
       "          [ 8.0149e-02,  1.0103e+00, -1.2418e-02,  ...,  1.7661e-02,\n",
       "           -6.4665e-03,  1.7678e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.6960e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-6.5539e-01,  3.3583e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-1.1978e-02, -3.6221e-01, -3.5958e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [ 7.9433e-02, -4.7285e-02,  2.5070e-01,  ...,  1.1319e-02,\n",
       "                  -inf,        -inf],\n",
       "          [ 1.9190e-01,  8.5144e-01,  1.8423e-01,  ..., -3.8147e-01,\n",
       "           -5.8918e-01,        -inf],\n",
       "          [-4.4065e-02,  6.4763e-02, -1.8268e-01,  ...,  1.0919e-01,\n",
       "           -2.0070e-01, -9.0524e-01]],\n",
       "\n",
       "         [[-5.9610e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-2.8451e-01, -1.7990e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 5.2701e-01, -6.4613e-01, -3.2664e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [ 1.2822e-01, -4.4832e-01, -3.5067e-01,  ...,  2.6362e-01,\n",
       "                  -inf,        -inf],\n",
       "          [ 2.6680e-02,  5.8386e-02, -6.9674e-01,  ..., -3.4030e-01,\n",
       "           -8.0955e-02,        -inf],\n",
       "          [ 1.2370e-02, -1.0646e-01, -6.7227e-01,  ..., -2.9729e-01,\n",
       "           -1.9717e-02, -5.9820e-01]],\n",
       "\n",
       "         [[-3.9291e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-3.8515e-01, -3.3707e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 2.1659e-01, -8.4501e-01, -1.0603e+00,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [ 3.9862e-01, -8.2333e-01, -1.0218e+00,  ..., -1.0384e-01,\n",
       "                  -inf,        -inf],\n",
       "          [ 1.7675e-01,  4.3847e-02,  2.2055e-01,  ...,  5.1665e-01,\n",
       "            6.3877e-02,        -inf],\n",
       "          [-1.4063e-01, -1.7225e-02, -9.8343e-03,  ...,  4.0647e-01,\n",
       "           -8.5126e-01,  1.3708e-01]],\n",
       "\n",
       "         [[ 1.5429e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 2.5917e-01, -1.7362e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-4.6128e-01, -1.0138e+00,  7.2450e-02,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [-2.3513e-01, -4.6902e-01,  7.6522e-01,  ...,  7.6387e-02,\n",
       "                  -inf,        -inf],\n",
       "          [-2.7501e-01,  2.9121e-01,  5.8520e-01,  ...,  5.6303e-01,\n",
       "            8.5436e-01,        -inf],\n",
       "          [-2.7687e-01,  2.0080e-01, -4.4961e-03,  ..., -1.2981e-01,\n",
       "            1.6379e-01, -4.2053e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.8309e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 1.1804e-01, -1.3512e+00,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 3.1705e-01, -1.2574e+00,  1.2254e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [-1.4185e-01, -6.9280e-01, -7.1766e-01,  ...,  7.3471e-02,\n",
       "                  -inf,        -inf],\n",
       "          [-4.0685e-01, -2.4381e-01,  8.5578e-02,  ...,  1.3404e-02,\n",
       "            1.7444e-01,        -inf],\n",
       "          [ 4.1147e-01, -8.3543e-01, -9.1430e-02,  ..., -1.2050e-01,\n",
       "            5.5010e-01,  3.7046e-01]],\n",
       "\n",
       "         [[-1.0508e+00,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-8.1576e-01,  7.8971e-02,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-1.1341e+00, -1.9326e-01, -6.6363e-02,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [-3.5235e-01, -1.5909e-01, -2.0895e-01,  ...,  3.7761e-01,\n",
       "                  -inf,        -inf],\n",
       "          [-8.0026e-01, -1.9847e-01, -3.0285e-01,  ...,  2.6411e-01,\n",
       "           -1.8758e-01,        -inf],\n",
       "          [-8.7323e-02, -1.5148e-01, -1.7110e-01,  ...,  7.1065e-01,\n",
       "            2.9210e-01, -3.3366e-01]],\n",
       "\n",
       "         [[-3.1714e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 1.4046e-01,  1.5642e-01,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 5.1328e-01,  6.9831e-02, -3.1491e-02,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [-1.2688e-01,  1.9160e-01, -3.0262e-01,  ..., -8.1260e-02,\n",
       "                  -inf,        -inf],\n",
       "          [-6.9087e-01, -5.4243e-01, -8.4824e-01,  ..., -6.2910e-01,\n",
       "           -7.5573e-01,        -inf],\n",
       "          [ 8.5977e-01,  6.2001e-01, -4.9563e-02,  ..., -2.8010e-01,\n",
       "            3.0581e-01, -9.3015e-02]],\n",
       "\n",
       "         [[ 4.9061e-01,        -inf,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [ 8.3791e-01,  2.0523e+00,        -inf,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          [-1.2503e-02,  6.4319e-01,  3.9919e-01,  ...,        -inf,\n",
       "                  -inf,        -inf],\n",
       "          ...,\n",
       "          [ 4.5132e-01,  6.1942e-01,  1.6204e-01,  ..., -1.6308e-01,\n",
       "                  -inf,        -inf],\n",
       "          [ 3.5854e-01,  1.4547e+00, -5.3177e-02,  ...,  7.4720e-01,\n",
       "           -1.4665e-01,        -inf],\n",
       "          [ 9.6652e-01,  1.3663e+00, -5.1990e-02,  ..., -2.9837e-01,\n",
       "           -5.5975e-01,  5.4050e-01]]]], grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor @\n",
    "output = Q @ K.transpose(-2,-1) / math.sqrt(d_model//num_heads) # transform token * token  & scale\n",
    "# # apply mask\n",
    "mask = torch.triu(torch.ones(context_length,context_length),diagonal=1).bool()\n",
    "output = output.masked_fill(mask,float('-inf')) # ignore predict section such as i am tom, here only i am\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3645, 0.6355, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3159, 0.3430, 0.3411,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0443, 0.0340, 0.0742,  ..., 0.1290, 0.0000, 0.0000],\n",
       "          [0.0552, 0.0331, 0.1450,  ..., 0.1282, 0.0827, 0.0000],\n",
       "          [0.0309, 0.0347, 0.0719,  ..., 0.1118, 0.1039, 0.0758]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5699, 0.4301, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2970, 0.3090, 0.3940,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0374, 0.0677, 0.0496,  ..., 0.0915, 0.0000, 0.0000],\n",
       "          [0.2032, 0.0473, 0.0470,  ..., 0.0609, 0.0397, 0.0000],\n",
       "          [0.0284, 0.0961, 0.0603,  ..., 0.0380, 0.0305, 0.0300]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4558, 0.5442, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2810, 0.3604, 0.3587,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0795, 0.0872, 0.0682,  ..., 0.0585, 0.0000, 0.0000],\n",
       "          [0.0497, 0.0781, 0.0363,  ..., 0.0592, 0.0772, 0.0000],\n",
       "          [0.0652, 0.0530, 0.0462,  ..., 0.0655, 0.0371, 0.1548]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4552, 0.5448, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2477, 0.2417, 0.5106,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0873, 0.0611, 0.0580,  ..., 0.0482, 0.0000, 0.0000],\n",
       "          [0.0434, 0.0420, 0.0693,  ..., 0.1047, 0.0594, 0.0000],\n",
       "          [0.0768, 0.0444, 0.0525,  ..., 0.0659, 0.0603, 0.0380]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4052, 0.5948, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2588, 0.2643, 0.4769,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0514, 0.0534, 0.0536,  ..., 0.0919, 0.0000, 0.0000],\n",
       "          [0.0619, 0.0473, 0.0698,  ..., 0.0596, 0.0645, 0.0000],\n",
       "          [0.0341, 0.0354, 0.0448,  ..., 0.0924, 0.0617, 0.0570]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4618, 0.5382, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2692, 0.2754, 0.4554,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0575, 0.0756, 0.0653,  ..., 0.1091, 0.0000, 0.0000],\n",
       "          [0.0373, 0.0385, 0.0522,  ..., 0.0797, 0.0575, 0.0000],\n",
       "          [0.0259, 0.0321, 0.0212,  ..., 0.0835, 0.0264, 0.0517]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5004, 0.4996, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1846, 0.3159, 0.4995,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1349, 0.0697, 0.0487,  ..., 0.0943, 0.0000, 0.0000],\n",
       "          [0.0461, 0.0635, 0.1131,  ..., 0.0260, 0.1175, 0.0000],\n",
       "          [0.1450, 0.0629, 0.0710,  ..., 0.0383, 0.0706, 0.0443]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3036, 0.6964, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2649, 0.3802, 0.3550,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0474, 0.0828, 0.0519,  ..., 0.1503, 0.0000, 0.0000],\n",
       "          [0.0688, 0.0723, 0.0670,  ..., 0.0886, 0.0678, 0.0000],\n",
       "          [0.0559, 0.1417, 0.0510,  ..., 0.0525, 0.0513, 0.0616]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2707, 0.7293, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4148, 0.2922, 0.2930,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0632, 0.0557, 0.0750,  ..., 0.0590, 0.0000, 0.0000],\n",
       "          [0.0662, 0.1280, 0.0657,  ..., 0.0373, 0.0303, 0.0000],\n",
       "          [0.0602, 0.0671, 0.0524,  ..., 0.0701, 0.0514, 0.0254]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4739, 0.5261, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5763, 0.1783, 0.2454,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0608, 0.0342, 0.0377,  ..., 0.0697, 0.0000, 0.0000],\n",
       "          [0.0699, 0.0722, 0.0339,  ..., 0.0484, 0.0628, 0.0000],\n",
       "          [0.0521, 0.0463, 0.0263,  ..., 0.0382, 0.0505, 0.0283]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4880, 0.5120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6155, 0.2129, 0.1716,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1386, 0.0408, 0.0335,  ..., 0.0838, 0.0000, 0.0000],\n",
       "          [0.0580, 0.0508, 0.0606,  ..., 0.0815, 0.0518, 0.0000],\n",
       "          [0.0635, 0.0718, 0.0724,  ..., 0.1097, 0.0312, 0.0838]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6065, 0.3935, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3048, 0.1754, 0.5198,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0493, 0.0390, 0.1340,  ..., 0.0673, 0.0000, 0.0000],\n",
       "          [0.0352, 0.0621, 0.0833,  ..., 0.0814, 0.1090, 0.0000],\n",
       "          [0.0512, 0.0825, 0.0672,  ..., 0.0593, 0.0795, 0.0443]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.8129, 0.1871, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4925, 0.1020, 0.4055,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0746, 0.0430, 0.0419,  ..., 0.0925, 0.0000, 0.0000],\n",
       "          [0.0405, 0.0477, 0.0663,  ..., 0.0616, 0.0724, 0.0000],\n",
       "          [0.0680, 0.0195, 0.0411,  ..., 0.0399, 0.0781, 0.0653]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2901, 0.7099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1545, 0.3959, 0.4495,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0533, 0.0646, 0.0615,  ..., 0.1106, 0.0000, 0.0000],\n",
       "          [0.0368, 0.0672, 0.0606,  ..., 0.1068, 0.0680, 0.0000],\n",
       "          [0.0435, 0.0408, 0.0400,  ..., 0.0966, 0.0635, 0.0340]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4960, 0.5040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4501, 0.2889, 0.2610,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0748, 0.1028, 0.0627,  ..., 0.0783, 0.0000, 0.0000],\n",
       "          [0.0773, 0.0896, 0.0660,  ..., 0.0822, 0.0724, 0.0000],\n",
       "          [0.1430, 0.1125, 0.0576,  ..., 0.0457, 0.0822, 0.0551]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2289, 0.7711, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2254, 0.4343, 0.3403,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0689, 0.0815, 0.0516,  ..., 0.0373, 0.0000, 0.0000],\n",
       "          [0.0526, 0.1576, 0.0349,  ..., 0.0777, 0.0318, 0.0000],\n",
       "          [0.0944, 0.1408, 0.0341,  ..., 0.0266, 0.0205, 0.0617]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply softmax\n",
    "attention_score = F.softmax(output,dim=-1) # score transform equal to [0,1] \n",
    "attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "A =  attention_score @ V #  here not use transpose(-2,-1) may be attention_score has the token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0398,  0.4818,  0.0963,  ...,  0.1679,  0.2287,  0.6676],\n",
       "         [-0.0863,  0.2418, -0.0479,  ..., -0.0519,  0.1836,  0.5052],\n",
       "         [-0.1069,  0.3283,  0.1011,  ..., -0.0190,  0.0015,  0.6509],\n",
       "         ...,\n",
       "         [ 0.2268, -0.0513, -0.3399,  ..., -0.1393, -0.0869,  0.1384],\n",
       "         [ 0.1988, -0.1359, -0.2928,  ..., -0.0974, -0.0921,  0.0950],\n",
       "         [ 0.2695, -0.0724, -0.3124,  ..., -0.1781, -0.1108,  0.0394]],\n",
       "\n",
       "        [[ 0.6502,  0.0718,  0.0830,  ..., -0.4850, -0.3659, -0.0209],\n",
       "         [ 0.3004,  0.1372, -0.0650,  ..., -0.2453, -0.0962,  0.3371],\n",
       "         [ 0.3175, -0.0472, -0.2013,  ..., -0.2026, -0.1292,  0.2658],\n",
       "         ...,\n",
       "         [ 0.2170, -0.1850, -0.4572,  ..., -0.0892, -0.1838, -0.0369],\n",
       "         [ 0.2548, -0.2806, -0.5512,  ..., -0.0559, -0.1754, -0.0636],\n",
       "         [ 0.2907, -0.2069, -0.4666,  ..., -0.0120, -0.1782, -0.1019]],\n",
       "\n",
       "        [[ 0.3059,  0.0707, -0.3349,  ...,  0.0121, -0.2921,  0.3436],\n",
       "         [ 0.2044,  0.3064, -0.6218,  ...,  0.0093, -0.3043,  0.0431],\n",
       "         [ 0.1219,  0.2719, -0.2903,  ...,  0.0674, -0.3643,  0.2957],\n",
       "         ...,\n",
       "         [ 0.3549, -0.2217, -0.5296,  ..., -0.0852, -0.1751, -0.1074],\n",
       "         [ 0.2993, -0.1732, -0.5430,  ..., -0.1018, -0.2057, -0.1050],\n",
       "         [ 0.2389, -0.1642, -0.4943,  ..., -0.0594, -0.1699, -0.0942]],\n",
       "\n",
       "        [[ 0.4730, -0.4894, -0.2456,  ..., -0.9469, -0.2850,  0.1021],\n",
       "         [ 0.4347, -0.3846, -0.1239,  ..., -0.9542,  0.0501,  0.4264],\n",
       "         [ 0.3316, -0.2349, -0.3333,  ..., -0.6011,  0.0128,  0.2343],\n",
       "         ...,\n",
       "         [ 0.2711, -0.3118, -0.6255,  ..., -0.2097, -0.2476, -0.0720],\n",
       "         [ 0.3030, -0.2745, -0.6920,  ..., -0.2301, -0.3090, -0.0470],\n",
       "         [ 0.3073, -0.3375, -0.6419,  ..., -0.1931, -0.2950, -0.0435]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate\n",
    "A = A.transpose(1,2).reshape(batch_size,-1,d_model)\n",
    "\n",
    "Wo = nn.Linear(d_model,d_model)\n",
    "output = Wo(A)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual connection\n",
    "output = output + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer normalization\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "layer_norm_output = layer_norm(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward network TODO: idk\n",
    "output = nn.Linear(d_model,d_model * 4)(layer_norm_output)\n",
    "output = nn.ReLU()(output)\n",
    "output = nn.Linear(d_model * 4,d_model)(output)\n",
    "\n",
    "output = output + layer_norm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer normalization\n",
    "output = layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 100070])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final liner layer\n",
    "output = nn.Linear(d_model,max_token_value+1)(output)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[9.1726e-06, 6.8594e-06, 3.7497e-06,  ..., 1.5194e-05,\n",
       "          8.9999e-06, 2.5022e-05],\n",
       "         [7.5116e-06, 7.2905e-06, 2.7375e-06,  ..., 1.1020e-05,\n",
       "          8.9646e-06, 1.1110e-05],\n",
       "         [8.6013e-06, 6.7701e-06, 1.1040e-05,  ..., 1.6123e-05,\n",
       "          2.4762e-05, 2.1542e-05],\n",
       "         ...,\n",
       "         [3.5195e-06, 4.5448e-06, 8.1129e-06,  ..., 5.1826e-06,\n",
       "          1.0819e-05, 1.1436e-05],\n",
       "         [5.4967e-06, 2.3233e-06, 3.5886e-06,  ..., 1.0309e-05,\n",
       "          6.7655e-06, 4.0197e-06],\n",
       "         [7.2078e-06, 4.1557e-06, 6.6817e-06,  ..., 1.5137e-05,\n",
       "          2.0015e-05, 2.3652e-06]],\n",
       "\n",
       "        [[6.5819e-06, 6.4126e-06, 3.9845e-06,  ..., 2.4906e-05,\n",
       "          8.9351e-06, 8.9222e-06],\n",
       "         [5.4351e-06, 7.4536e-06, 3.7279e-06,  ..., 1.6725e-05,\n",
       "          6.8678e-06, 7.2662e-06],\n",
       "         [7.1137e-06, 1.5165e-05, 3.8613e-06,  ..., 1.4368e-05,\n",
       "          1.2998e-05, 1.2725e-05],\n",
       "         ...,\n",
       "         [7.7805e-06, 4.1522e-06, 4.7183e-06,  ..., 8.6784e-06,\n",
       "          5.2076e-06, 3.5300e-06],\n",
       "         [7.3743e-06, 1.0633e-05, 4.9740e-06,  ..., 1.2242e-05,\n",
       "          7.5886e-06, 6.3455e-06],\n",
       "         [1.1813e-05, 7.4392e-06, 9.9978e-06,  ..., 1.5591e-05,\n",
       "          8.4124e-06, 4.6529e-06]],\n",
       "\n",
       "        [[1.0094e-05, 6.5327e-06, 7.2224e-06,  ..., 9.8244e-06,\n",
       "          7.0995e-06, 2.1217e-05],\n",
       "         [7.1031e-06, 9.1587e-06, 2.6498e-06,  ..., 9.3741e-06,\n",
       "          9.6412e-06, 1.9900e-05],\n",
       "         [8.2783e-06, 7.8732e-06, 6.5933e-06,  ..., 5.4519e-06,\n",
       "          1.5721e-05, 1.7781e-05],\n",
       "         ...,\n",
       "         [1.0471e-05, 6.6476e-06, 8.7420e-06,  ..., 1.8471e-05,\n",
       "          4.7554e-06, 1.0304e-05],\n",
       "         [2.1021e-05, 7.1864e-06, 4.6436e-06,  ..., 1.0202e-05,\n",
       "          6.3235e-06, 9.3981e-06],\n",
       "         [1.7543e-05, 1.1606e-05, 1.1337e-05,  ..., 9.3758e-06,\n",
       "          1.0763e-05, 2.8752e-06]],\n",
       "\n",
       "        [[1.2213e-05, 8.8765e-06, 4.7508e-06,  ..., 3.5363e-06,\n",
       "          7.1531e-06, 1.0694e-05],\n",
       "         [1.2229e-05, 9.3874e-06, 3.7976e-06,  ..., 1.2194e-05,\n",
       "          4.3235e-06, 9.3567e-06],\n",
       "         [1.3718e-05, 1.5677e-05, 4.5420e-06,  ..., 5.6146e-06,\n",
       "          1.0094e-05, 4.3678e-06],\n",
       "         ...,\n",
       "         [1.6262e-05, 1.2621e-05, 6.4333e-06,  ..., 7.4710e-06,\n",
       "          6.3059e-06, 9.6150e-06],\n",
       "         [7.2507e-06, 6.6402e-06, 1.4516e-05,  ..., 8.0595e-06,\n",
       "          9.1936e-06, 4.3351e-06],\n",
       "         [5.4879e-06, 8.7765e-06, 1.5970e-05,  ..., 1.1927e-05,\n",
       "          4.1115e-06, 3.4748e-06]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = F.softmax(output,dim=-1)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ocular'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(logits[0,0]))\n",
    "\n",
    "predicted_idx = torch.argmax(logits[0,0]).item() # get max score index\n",
    "\n",
    "encoding.decode([predicted_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
