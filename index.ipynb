{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get datasets\n",
    "if not os.path.exists('sales_textbook.txt'):\n",
    "  url = 'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/resolve/main/sales_textbook.txt?download=true'\n",
    "  with open('sales_textbook.txt','wb') as f:\n",
    "    f.write(requests.get(url).content)\n",
    "\n",
    "# read content to memory\n",
    "with open('sales_textbook.txt','r') as f:\n",
    "  text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77919"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize origin datasets\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "tokenized_text = encoding.encode(text)\n",
    "# list to tensor\n",
    "tokenized_text = torch.tensor(tokenized_text,dtype=torch.long)\n",
    "max_token_value = tokenized_text.max().item()\n",
    "len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26072,   220,    16,  ...,  1501, 48451,  7119])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train sets and validate sets\n",
    "train_idx = int(len(tokenized_text) * 0.9)\n",
    "train_data = tokenized_text[:train_idx]\n",
    "valid_data = tokenized_text[train_idx:]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "context_length = 16 # split input ant the input include 16 token\n",
    "d_model = 64\n",
    "batch_size = 4 # 4 train parallel\n",
    "num_heads = 4 # multi head num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1139,   279, 16188, 13189,   315,  8830,   279,  6444,     6,  3966,\n",
       "           323,  6784,  3585,    13,  1115, 16996],\n",
       "        [   13, 81745, 25363, 36870, 10758,  7512,   374,   264, 77975,  1920,\n",
       "           430,  7612,  6725,    11, 11302,    11],\n",
       "        [  449,   701, 10877,    11,   499,   649, 16988,   872, 21958,    11,\n",
       "         51077,   872, 28899,    11,   323,  5387],\n",
       "        [12207, 18885,  4642, 14624,  7512,   927,   892,   627,   644, 17102,\n",
       "            11,  4642, 14624,   374,   264, 89328]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train datastruct should be [4,16,64] 4 batch 16 token 64 dimension\n",
    "data = train_data\n",
    "idxs = torch.randint(low=0,high=len(data)-context_length,size=(batch_size,)) # rand 4 batch start index position\n",
    "x_batch = torch.stack([data[idx:idx+context_length] for idx in idxs])\n",
    "y_batch = torch.stack([data[idx+1:idx+context_length+1] for idx in idxs])\n",
    "y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 64])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input embedding table\n",
    "# row express One of 16 tokens, column express 64 dimension\n",
    "# step 1 : create a full table row of all token, and column is 64 dimension\n",
    "input_embedding_lookup_table = nn.Embedding(max_token_value + 1,d_model)\n",
    "\n",
    "x_batch_embedding = input_embedding_lookup_table(x_batch)\n",
    "y_batch_embedding = input_embedding_lookup_table(y_batch)\n",
    "x_batch_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding sin cos TODO: here idk so copy paper\n",
    "position_encoding_lookup_table = torch.zeros(context_length,d_model)\n",
    "position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "# apply the sine & cosine\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)\n",
    "position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)\n",
    "position_encoding_lookup_table = position_encoding_lookup_table.unsqueeze(0).expand(batch_size, -1, -1) # add batch to the first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009514</td>\n",
       "      <td>-0.628647</td>\n",
       "      <td>0.453952</td>\n",
       "      <td>-0.693385</td>\n",
       "      <td>-0.392355</td>\n",
       "      <td>1.039178</td>\n",
       "      <td>0.393323</td>\n",
       "      <td>1.536048</td>\n",
       "      <td>-2.246852</td>\n",
       "      <td>1.840393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029328</td>\n",
       "      <td>0.325352</td>\n",
       "      <td>1.426127</td>\n",
       "      <td>3.196872</td>\n",
       "      <td>0.181568</td>\n",
       "      <td>2.140222</td>\n",
       "      <td>0.856732</td>\n",
       "      <td>2.353050</td>\n",
       "      <td>1.329569</td>\n",
       "      <td>1.651698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.044880</td>\n",
       "      <td>1.463692</td>\n",
       "      <td>0.424040</td>\n",
       "      <td>0.575627</td>\n",
       "      <td>2.172730</td>\n",
       "      <td>2.429986</td>\n",
       "      <td>0.185697</td>\n",
       "      <td>1.636480</td>\n",
       "      <td>-0.394112</td>\n",
       "      <td>0.741410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840068</td>\n",
       "      <td>1.271349</td>\n",
       "      <td>-0.106082</td>\n",
       "      <td>0.573165</td>\n",
       "      <td>-1.227755</td>\n",
       "      <td>0.144123</td>\n",
       "      <td>-0.628551</td>\n",
       "      <td>-0.643179</td>\n",
       "      <td>1.833568</td>\n",
       "      <td>0.420245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.186652</td>\n",
       "      <td>0.540543</td>\n",
       "      <td>1.420622</td>\n",
       "      <td>-0.821325</td>\n",
       "      <td>1.654497</td>\n",
       "      <td>2.488250</td>\n",
       "      <td>0.513854</td>\n",
       "      <td>-0.012210</td>\n",
       "      <td>2.487984</td>\n",
       "      <td>1.625387</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.426523</td>\n",
       "      <td>0.695596</td>\n",
       "      <td>-1.201532</td>\n",
       "      <td>1.859443</td>\n",
       "      <td>-0.280295</td>\n",
       "      <td>1.759851</td>\n",
       "      <td>0.536200</td>\n",
       "      <td>1.185389</td>\n",
       "      <td>-0.409233</td>\n",
       "      <td>2.586292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.139578</td>\n",
       "      <td>-1.003603</td>\n",
       "      <td>1.547197</td>\n",
       "      <td>-1.083511</td>\n",
       "      <td>-0.713684</td>\n",
       "      <td>0.546100</td>\n",
       "      <td>-0.324615</td>\n",
       "      <td>-0.173023</td>\n",
       "      <td>0.566914</td>\n",
       "      <td>0.390260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.726410</td>\n",
       "      <td>1.038835</td>\n",
       "      <td>-2.591173</td>\n",
       "      <td>1.021517</td>\n",
       "      <td>1.101539</td>\n",
       "      <td>0.925657</td>\n",
       "      <td>-0.786015</td>\n",
       "      <td>-0.014163</td>\n",
       "      <td>-0.087064</td>\n",
       "      <td>1.004659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.049731</td>\n",
       "      <td>0.043569</td>\n",
       "      <td>-0.268855</td>\n",
       "      <td>-0.236893</td>\n",
       "      <td>0.258206</td>\n",
       "      <td>1.006418</td>\n",
       "      <td>2.750966</td>\n",
       "      <td>0.706177</td>\n",
       "      <td>2.160859</td>\n",
       "      <td>1.020835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421504</td>\n",
       "      <td>1.139157</td>\n",
       "      <td>0.258226</td>\n",
       "      <td>1.917334</td>\n",
       "      <td>-1.025656</td>\n",
       "      <td>2.881215</td>\n",
       "      <td>-0.037726</td>\n",
       "      <td>2.391368</td>\n",
       "      <td>-0.054268</td>\n",
       "      <td>1.352070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.329391</td>\n",
       "      <td>0.875787</td>\n",
       "      <td>-1.437143</td>\n",
       "      <td>0.175070</td>\n",
       "      <td>-0.779667</td>\n",
       "      <td>-0.361700</td>\n",
       "      <td>1.139137</td>\n",
       "      <td>0.736458</td>\n",
       "      <td>0.874199</td>\n",
       "      <td>1.042657</td>\n",
       "      <td>...</td>\n",
       "      <td>1.179075</td>\n",
       "      <td>-0.056524</td>\n",
       "      <td>-0.669338</td>\n",
       "      <td>1.493032</td>\n",
       "      <td>-1.126546</td>\n",
       "      <td>0.748562</td>\n",
       "      <td>0.533761</td>\n",
       "      <td>1.701273</td>\n",
       "      <td>-0.498121</td>\n",
       "      <td>0.812271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.817968</td>\n",
       "      <td>-0.275213</td>\n",
       "      <td>-0.938052</td>\n",
       "      <td>0.741385</td>\n",
       "      <td>-0.943942</td>\n",
       "      <td>-0.752198</td>\n",
       "      <td>0.279236</td>\n",
       "      <td>-2.377340</td>\n",
       "      <td>2.462071</td>\n",
       "      <td>0.851587</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.110642</td>\n",
       "      <td>3.283916</td>\n",
       "      <td>2.338055</td>\n",
       "      <td>-0.062029</td>\n",
       "      <td>-1.372887</td>\n",
       "      <td>3.070702</td>\n",
       "      <td>-1.518887</td>\n",
       "      <td>0.345257</td>\n",
       "      <td>1.796947</td>\n",
       "      <td>0.146887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.065659</td>\n",
       "      <td>1.710592</td>\n",
       "      <td>-0.436171</td>\n",
       "      <td>-0.380824</td>\n",
       "      <td>0.038645</td>\n",
       "      <td>1.356357</td>\n",
       "      <td>-0.044468</td>\n",
       "      <td>-1.659200</td>\n",
       "      <td>2.697279</td>\n",
       "      <td>0.219371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.424414</td>\n",
       "      <td>0.695592</td>\n",
       "      <td>-1.199951</td>\n",
       "      <td>1.859441</td>\n",
       "      <td>-0.279110</td>\n",
       "      <td>1.759850</td>\n",
       "      <td>0.537089</td>\n",
       "      <td>1.185388</td>\n",
       "      <td>-0.408566</td>\n",
       "      <td>2.586291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.504581</td>\n",
       "      <td>-1.383468</td>\n",
       "      <td>-2.202643</td>\n",
       "      <td>0.008550</td>\n",
       "      <td>-1.016984</td>\n",
       "      <td>0.619911</td>\n",
       "      <td>1.188565</td>\n",
       "      <td>-0.495199</td>\n",
       "      <td>1.783550</td>\n",
       "      <td>-1.712065</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261004</td>\n",
       "      <td>1.194813</td>\n",
       "      <td>0.728480</td>\n",
       "      <td>0.825866</td>\n",
       "      <td>0.950326</td>\n",
       "      <td>1.124104</td>\n",
       "      <td>-0.349640</td>\n",
       "      <td>-0.299419</td>\n",
       "      <td>-1.133275</td>\n",
       "      <td>2.203186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.125427</td>\n",
       "      <td>0.285392</td>\n",
       "      <td>2.284018</td>\n",
       "      <td>2.042556</td>\n",
       "      <td>0.599608</td>\n",
       "      <td>0.544710</td>\n",
       "      <td>0.628323</td>\n",
       "      <td>-0.510135</td>\n",
       "      <td>1.131524</td>\n",
       "      <td>-1.588181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111559</td>\n",
       "      <td>2.023352</td>\n",
       "      <td>0.155175</td>\n",
       "      <td>1.203321</td>\n",
       "      <td>-0.119158</td>\n",
       "      <td>1.648694</td>\n",
       "      <td>0.761911</td>\n",
       "      <td>0.502889</td>\n",
       "      <td>-2.132461</td>\n",
       "      <td>0.572767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.583278</td>\n",
       "      <td>-0.565864</td>\n",
       "      <td>-0.055896</td>\n",
       "      <td>-0.286747</td>\n",
       "      <td>0.177072</td>\n",
       "      <td>1.500578</td>\n",
       "      <td>-2.389148</td>\n",
       "      <td>-0.207169</td>\n",
       "      <td>1.040118</td>\n",
       "      <td>-0.491852</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190399</td>\n",
       "      <td>-0.813175</td>\n",
       "      <td>0.692895</td>\n",
       "      <td>1.096872</td>\n",
       "      <td>0.164004</td>\n",
       "      <td>1.068765</td>\n",
       "      <td>0.745296</td>\n",
       "      <td>0.622972</td>\n",
       "      <td>-1.180538</td>\n",
       "      <td>0.530390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.145032</td>\n",
       "      <td>-1.628992</td>\n",
       "      <td>1.300162</td>\n",
       "      <td>0.798610</td>\n",
       "      <td>-0.787733</td>\n",
       "      <td>1.987257</td>\n",
       "      <td>-1.368982</td>\n",
       "      <td>-0.533858</td>\n",
       "      <td>0.302204</td>\n",
       "      <td>-0.298272</td>\n",
       "      <td>...</td>\n",
       "      <td>1.891402</td>\n",
       "      <td>0.695575</td>\n",
       "      <td>-1.021317</td>\n",
       "      <td>1.178368</td>\n",
       "      <td>-0.598476</td>\n",
       "      <td>0.946472</td>\n",
       "      <td>0.544932</td>\n",
       "      <td>-0.784374</td>\n",
       "      <td>-0.524200</td>\n",
       "      <td>1.039736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.329630</td>\n",
       "      <td>1.053351</td>\n",
       "      <td>-0.924517</td>\n",
       "      <td>-0.151945</td>\n",
       "      <td>-0.744923</td>\n",
       "      <td>1.664144</td>\n",
       "      <td>-1.393224</td>\n",
       "      <td>1.263280</td>\n",
       "      <td>-1.005657</td>\n",
       "      <td>-0.708942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.745576</td>\n",
       "      <td>1.352647</td>\n",
       "      <td>-0.292892</td>\n",
       "      <td>2.175398</td>\n",
       "      <td>-0.766818</td>\n",
       "      <td>2.550579</td>\n",
       "      <td>-0.777749</td>\n",
       "      <td>0.665159</td>\n",
       "      <td>-0.592906</td>\n",
       "      <td>1.695828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.466644</td>\n",
       "      <td>1.469923</td>\n",
       "      <td>2.463877</td>\n",
       "      <td>-3.024180</td>\n",
       "      <td>-0.570247</td>\n",
       "      <td>2.621802</td>\n",
       "      <td>-1.399389</td>\n",
       "      <td>0.323436</td>\n",
       "      <td>-1.304813</td>\n",
       "      <td>-0.878870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011560</td>\n",
       "      <td>-1.751208</td>\n",
       "      <td>1.214717</td>\n",
       "      <td>0.765260</td>\n",
       "      <td>-0.104693</td>\n",
       "      <td>0.758693</td>\n",
       "      <td>-0.591876</td>\n",
       "      <td>1.024271</td>\n",
       "      <td>0.676459</td>\n",
       "      <td>1.725315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.051315</td>\n",
       "      <td>-0.424745</td>\n",
       "      <td>0.234548</td>\n",
       "      <td>-1.423722</td>\n",
       "      <td>2.082680</td>\n",
       "      <td>1.639610</td>\n",
       "      <td>0.617815</td>\n",
       "      <td>2.075426</td>\n",
       "      <td>-1.469917</td>\n",
       "      <td>-0.892302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.883735</td>\n",
       "      <td>1.553876</td>\n",
       "      <td>-0.319996</td>\n",
       "      <td>0.216916</td>\n",
       "      <td>-0.220743</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.678886</td>\n",
       "      <td>1.414718</td>\n",
       "      <td>1.171395</td>\n",
       "      <td>0.290097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.077184</td>\n",
       "      <td>-0.739441</td>\n",
       "      <td>-2.429254</td>\n",
       "      <td>-0.364286</td>\n",
       "      <td>1.687256</td>\n",
       "      <td>-1.447924</td>\n",
       "      <td>1.185209</td>\n",
       "      <td>2.235366</td>\n",
       "      <td>-0.905539</td>\n",
       "      <td>-1.409799</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.338530</td>\n",
       "      <td>0.660423</td>\n",
       "      <td>0.895289</td>\n",
       "      <td>0.519565</td>\n",
       "      <td>1.049080</td>\n",
       "      <td>0.595433</td>\n",
       "      <td>-1.549783</td>\n",
       "      <td>0.466050</td>\n",
       "      <td>-0.476400</td>\n",
       "      <td>1.378834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.009514 -0.628647  0.453952 -0.693385 -0.392355  1.039178  0.393323   \n",
       "1   1.044880  1.463692  0.424040  0.575627  2.172730  2.429986  0.185697   \n",
       "2   0.186652  0.540543  1.420622 -0.821325  1.654497  2.488250  0.513854   \n",
       "3  -0.139578 -1.003603  1.547197 -1.083511 -0.713684  0.546100 -0.324615   \n",
       "4  -3.049731  0.043569 -0.268855 -0.236893  0.258206  1.006418  2.750966   \n",
       "5  -0.329391  0.875787 -1.437143  0.175070 -0.779667 -0.361700  1.139137   \n",
       "6  -0.817968 -0.275213 -0.938052  0.741385 -0.943942 -0.752198  0.279236   \n",
       "7  -0.065659  1.710592 -0.436171 -0.380824  0.038645  1.356357 -0.044468   \n",
       "8   0.504581 -1.383468 -2.202643  0.008550 -1.016984  0.619911  1.188565   \n",
       "9   0.125427  0.285392  2.284018  2.042556  0.599608  0.544710  0.628323   \n",
       "10  0.583278 -0.565864 -0.055896 -0.286747  0.177072  1.500578 -2.389148   \n",
       "11 -0.145032 -1.628992  1.300162  0.798610 -0.787733  1.987257 -1.368982   \n",
       "12  0.329630  1.053351 -0.924517 -0.151945 -0.744923  1.664144 -1.393224   \n",
       "13  0.466644  1.469923  2.463877 -3.024180 -0.570247  2.621802 -1.399389   \n",
       "14  1.051315 -0.424745  0.234548 -1.423722  2.082680  1.639610  0.617815   \n",
       "15  0.077184 -0.739441 -2.429254 -0.364286  1.687256 -1.447924  1.185209   \n",
       "\n",
       "          7         8         9   ...        54        55        56        57  \\\n",
       "0   1.536048 -2.246852  1.840393  ...  0.029328  0.325352  1.426127  3.196872   \n",
       "1   1.636480 -0.394112  0.741410  ...  0.840068  1.271349 -0.106082  0.573165   \n",
       "2  -0.012210  2.487984  1.625387  ... -0.426523  0.695596 -1.201532  1.859443   \n",
       "3  -0.173023  0.566914  0.390260  ... -0.726410  1.038835 -2.591173  1.021517   \n",
       "4   0.706177  2.160859  1.020835  ...  0.421504  1.139157  0.258226  1.917334   \n",
       "5   0.736458  0.874199  1.042657  ...  1.179075 -0.056524 -0.669338  1.493032   \n",
       "6  -2.377340  2.462071  0.851587  ... -0.110642  3.283916  2.338055 -0.062029   \n",
       "7  -1.659200  2.697279  0.219371  ... -0.424414  0.695592 -1.199951  1.859441   \n",
       "8  -0.495199  1.783550 -1.712065  ... -0.261004  1.194813  0.728480  0.825866   \n",
       "9  -0.510135  1.131524 -1.588181  ...  0.111559  2.023352  0.155175  1.203321   \n",
       "10 -0.207169  1.040118 -0.491852  ... -0.190399 -0.813175  0.692895  1.096872   \n",
       "11 -0.533858  0.302204 -0.298272  ...  1.891402  0.695575 -1.021317  1.178368   \n",
       "12  1.263280 -1.005657 -0.708942  ... -0.745576  1.352647 -0.292892  2.175398   \n",
       "13  0.323436 -1.304813 -0.878870  ...  0.011560 -1.751208  1.214717  0.765260   \n",
       "14  2.075426 -1.469917 -0.892302  ...  0.883735  1.553876 -0.319996  0.216916   \n",
       "15  2.235366 -0.905539 -1.409799  ... -0.338530  0.660423  0.895289  0.519565   \n",
       "\n",
       "          58        59        60        61        62        63  \n",
       "0   0.181568  2.140222  0.856732  2.353050  1.329569  1.651698  \n",
       "1  -1.227755  0.144123 -0.628551 -0.643179  1.833568  0.420245  \n",
       "2  -0.280295  1.759851  0.536200  1.185389 -0.409233  2.586292  \n",
       "3   1.101539  0.925657 -0.786015 -0.014163 -0.087064  1.004659  \n",
       "4  -1.025656  2.881215 -0.037726  2.391368 -0.054268  1.352070  \n",
       "5  -1.126546  0.748562  0.533761  1.701273 -0.498121  0.812271  \n",
       "6  -1.372887  3.070702 -1.518887  0.345257  1.796947  0.146887  \n",
       "7  -0.279110  1.759850  0.537089  1.185388 -0.408566  2.586291  \n",
       "8   0.950326  1.124104 -0.349640 -0.299419 -1.133275  2.203186  \n",
       "9  -0.119158  1.648694  0.761911  0.502889 -2.132461  0.572767  \n",
       "10  0.164004  1.068765  0.745296  0.622972 -1.180538  0.530390  \n",
       "11 -0.598476  0.946472  0.544932 -0.784374 -0.524200  1.039736  \n",
       "12 -0.766818  2.550579 -0.777749  0.665159 -0.592906  1.695828  \n",
       "13 -0.104693  0.758693 -0.591876  1.024271  0.676459  1.725315  \n",
       "14 -0.220743  0.010941  0.678886  1.414718  1.171395  0.290097  \n",
       "15  1.049080  0.595433 -1.549783  0.466050 -0.476400  1.378834  \n",
       "\n",
       "[16 rows x 64 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add positional encoding to the input embedding\n",
    "x = x_batch_embedding + position_encoding_lookup_table\n",
    "y = y_batch_embedding + position_encoding_lookup_table\n",
    "pd.DataFrame(x[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 16, 64]), torch.Size([4, 16, 64]), torch.Size([4, 16, 64]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate Q,K,V\n",
    "Wq = nn.Linear(d_model,d_model)\n",
    "Wk = nn.Linear(d_model,d_model)\n",
    "Wv = nn.Linear(d_model,d_model)\n",
    "# default last [16 ,64] * [64,64] => [16,64], execute 4 batch auto\n",
    "Q = Wq(x)\n",
    "K = Wk(x)\n",
    "V = Wv(x)\n",
    "Q.shape,K.shape,V.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 16, 16])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply multi-head\n",
    "Q = Q.reshape(batch_size,context_length,num_heads,d_model//num_heads)\n",
    "K = K.reshape(batch_size,context_length,num_heads,d_model//num_heads)\n",
    "V = V.reshape(batch_size,context_length,num_heads,d_model//num_heads)\n",
    "\n",
    "Q = Q.permute(0,2,1,3) # [4,16,4,16] to [4,4,16,16]\n",
    "K = K.permute(0,2,1,3) # [4,16,4,16] to [4,4,16,16]\n",
    "V = V.permute(0,2,1,3) # [4,16,4,16] to [4,4,16,16]\n",
    "Q.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3606,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.3952, -0.7748,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.5949, -0.1296, -0.1801,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.4082, -0.0957, -0.1093,  ...,  0.3589,    -inf,    -inf],\n",
       "          [-0.2295, -0.0230, -0.3824,  ...,  1.1017,  0.2625,    -inf],\n",
       "          [ 0.4246,  0.2365, -0.1537,  ...,  0.9713,  0.3656, -0.0315]],\n",
       "\n",
       "         [[-1.2565,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.6804,  0.0429,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.4348, -0.1344, -0.5002,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.6650, -0.9835, -1.3109,  ..., -1.0226,    -inf,    -inf],\n",
       "          [ 0.5416,  0.1189,  0.1980,  ...,  1.0053, -0.1624,    -inf],\n",
       "          [ 0.7271, -0.0145, -0.3190,  ...,  0.6860, -0.2814, -0.0558]],\n",
       "\n",
       "         [[ 0.5502,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.5849,  0.3819,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.5753,  0.3856,  0.2018,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.2304,  0.3315, -0.1444,  ..., -0.3688,    -inf,    -inf],\n",
       "          [ 0.7863,  0.2036,  0.2563,  ...,  0.6995, -0.4562,    -inf],\n",
       "          [ 0.3235,  0.0897,  0.1014,  ...,  0.3241,  0.0193, -0.1306]],\n",
       "\n",
       "         [[-0.2257,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.1444, -0.4064,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.5706, -0.1885, -0.3241,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.3961, -0.0707,  0.1983,  ..., -0.2068,    -inf,    -inf],\n",
       "          [ 0.9485,  0.1841,  0.0502,  ...,  0.3180,  0.3374,    -inf],\n",
       "          [ 0.6325,  0.3144,  0.2862,  ...,  0.3792,  0.6433,  0.6044]]],\n",
       "\n",
       "\n",
       "        [[[ 0.8519,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.3823, -0.3645,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.1023, -0.0845,  0.1526,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.0124, -0.1967, -0.6525,  ..., -0.0590,    -inf,    -inf],\n",
       "          [ 0.7525,  0.3466,  0.1761,  ...,  0.3439,  0.5306,    -inf],\n",
       "          [ 0.2414, -0.0295,  0.0066,  ...,  0.2196,  0.6861,  0.1715]],\n",
       "\n",
       "         [[-0.8029,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.2040, -0.6363,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.1401, -0.9153, -1.1492,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.7152, -0.5090, -1.0892,  ..., -1.4381,    -inf,    -inf],\n",
       "          [ 0.4441,  0.9730,  0.7342,  ..., -0.4718,  0.2501,    -inf],\n",
       "          [ 0.7566, -0.3772, -1.3777,  ..., -1.3736,  0.1332, -0.4297]],\n",
       "\n",
       "         [[-0.0419,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.6108, -0.2295,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.2753,  0.0551,  0.6153,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.1058, -0.4827,  0.1165,  ...,  0.3363,    -inf,    -inf],\n",
       "          [ 0.5657,  1.0388,  0.4310,  ...,  0.6985,  0.0029,    -inf],\n",
       "          [-0.2808,  0.1790,  0.8144,  ...,  0.0578, -1.0301, -0.0898]],\n",
       "\n",
       "         [[ 0.0170,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.5219,  0.0713,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.0724, -0.6454, -0.1249,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 1.1372,  0.4413, -0.0223,  ...,  1.1544,    -inf,    -inf],\n",
       "          [-0.9707, -0.9468, -0.4242,  ..., -0.7292, -0.0517,    -inf],\n",
       "          [-0.1177, -0.7020, -0.2925,  ...,  0.0893, -0.5468, -0.6073]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5363,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.0785, -0.5131,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.0567, -0.3989, -0.3757,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.2264, -0.4692, -0.4848,  ...,  0.0372,    -inf,    -inf],\n",
       "          [ 0.3801,  0.2125,  0.2469,  ...,  0.1247,  0.5306,    -inf],\n",
       "          [-0.4302, -0.1087,  0.6969,  ..., -0.3615, -0.0632, -0.2790]],\n",
       "\n",
       "         [[-0.8732,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.7363, -0.8614,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.6546, -1.0855,  0.1796,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-1.0376, -0.0066,  0.3080,  ..., -0.6030,    -inf,    -inf],\n",
       "          [ 0.1762,  1.0077,  0.7943,  ...,  1.0603,  0.2501,    -inf],\n",
       "          [ 0.2324, -0.7058, -0.1803,  ...,  0.2073, -0.1115, -0.4086]],\n",
       "\n",
       "         [[-0.3696,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.0763,  0.0968,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.5037,  0.1750,  0.6611,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.4460,  0.5513,  0.3545,  ...,  0.2537,    -inf,    -inf],\n",
       "          [ 1.0614,  0.8593,  0.8989,  ...,  0.5446,  0.0029,    -inf],\n",
       "          [ 0.1825, -0.1130,  0.2721,  ..., -0.2880, -0.1598, -0.3545]],\n",
       "\n",
       "         [[-1.5619,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.8192, -0.6699,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-1.4025, -0.8452, -0.2617,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.0508, -0.4864,  0.4938,  ...,  0.2153,    -inf,    -inf],\n",
       "          [-1.0665, -0.0697, -1.4825,  ..., -0.3534, -0.0517,    -inf],\n",
       "          [-1.4279, -0.1883, -0.7456,  ..., -0.3828, -0.0427, -0.5414]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3450,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.4477, -1.2459,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.3698, -0.7466, -0.1087,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.0441, -0.1625,  0.4618,  ...,  0.2222,    -inf,    -inf],\n",
       "          [-0.7763, -0.7560, -0.3265,  ...,  0.2225, -0.4948,    -inf],\n",
       "          [ 0.0249, -1.7672, -0.9226,  ...,  0.8143,  0.4202, -0.2131]],\n",
       "\n",
       "         [[-0.4467,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-1.2390, -1.7099,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.6619, -0.9940,  0.2086,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.3182,  1.2866,  0.0877,  ..., -0.3947,    -inf,    -inf],\n",
       "          [-0.0036, -0.6379, -0.5325,  ..., -0.6776,  0.7095,    -inf],\n",
       "          [-0.8135,  0.1113,  0.5320,  ..., -1.0600,  0.8175, -1.0863]],\n",
       "\n",
       "         [[-0.0060,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.3481,  0.7293,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.0232,  0.6768,  0.0108,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 1.0293,  0.8469,  0.0500,  ...,  0.3315,    -inf,    -inf],\n",
       "          [-0.2639,  0.6936, -0.0513,  ..., -0.5041, -0.7900,    -inf],\n",
       "          [ 0.2063,  0.7804,  0.6126,  ..., -0.6616,  0.3890, -0.4306]],\n",
       "\n",
       "         [[ 0.0192,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.1004, -0.7380,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.6636, -0.0942, -0.2919,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.4058, -0.2741,  0.1601,  ..., -0.4965,    -inf,    -inf],\n",
       "          [ 0.7542, -0.3455, -0.3267,  ...,  1.0759,  0.7381,    -inf],\n",
       "          [ 0.3690, -0.1432,  0.2260,  ..., -0.2271, -0.8130, -0.3289]]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor @\n",
    "output = Q @ K.transpose(-2,-1) / math.sqrt(d_model//num_heads) # transform token * token  & scale\n",
    "# # apply mask\n",
    "mask = torch.triu(torch.ones(context_length,context_length),diagonal=1).bool()\n",
    "output = output.masked_fill(mask,float('-inf')) # ignore predict section such as i am tom, here only i am\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5938, 0.4062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2435, 0.3878, 0.3687,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0435, 0.0594, 0.0586,  ..., 0.0936, 0.0000, 0.0000],\n",
       "          [0.0489, 0.0601, 0.0420,  ..., 0.1852, 0.0800, 0.0000],\n",
       "          [0.0910, 0.0754, 0.0510,  ..., 0.1571, 0.0858, 0.0576]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3267, 0.6733, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3042, 0.4108, 0.2850,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0747, 0.0543, 0.0392,  ..., 0.0522, 0.0000, 0.0000],\n",
       "          [0.1181, 0.0774, 0.0837,  ..., 0.1877, 0.0584, 0.0000],\n",
       "          [0.1636, 0.0779, 0.0575,  ..., 0.1570, 0.0597, 0.0748]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5506, 0.4494, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3975, 0.3288, 0.2736,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1004, 0.1111, 0.0690,  ..., 0.0552, 0.0000, 0.0000],\n",
       "          [0.1035, 0.0578, 0.0609,  ..., 0.0949, 0.0299, 0.0000],\n",
       "          [0.0874, 0.0692, 0.0700,  ..., 0.0875, 0.0645, 0.0555]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5651, 0.4349, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2670, 0.3913, 0.3417,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0941, 0.0590, 0.0772,  ..., 0.0515, 0.0000, 0.0000],\n",
       "          [0.1260, 0.0587, 0.0513,  ..., 0.0671, 0.0684, 0.0000],\n",
       "          [0.0836, 0.0608, 0.0591,  ..., 0.0649, 0.0845, 0.0813]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4956, 0.5044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3023, 0.3077, 0.3900,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0897, 0.0746, 0.0473,  ..., 0.0856, 0.0000, 0.0000],\n",
       "          [0.1069, 0.0712, 0.0601,  ..., 0.0710, 0.0856, 0.0000],\n",
       "          [0.0661, 0.0504, 0.0523,  ..., 0.0647, 0.1031, 0.0616]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6064, 0.3936, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6160, 0.2144, 0.1697,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1455, 0.0428, 0.0239,  ..., 0.0169, 0.0000, 0.0000],\n",
       "          [0.0783, 0.1328, 0.1046,  ..., 0.0313, 0.0645, 0.0000],\n",
       "          [0.1621, 0.0521, 0.0192,  ..., 0.0193, 0.0869, 0.0495]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4058, 0.5942, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2071, 0.2882, 0.5047,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0401, 0.0275, 0.0500,  ..., 0.0623, 0.0000, 0.0000],\n",
       "          [0.0674, 0.1082, 0.0589,  ..., 0.0770, 0.0384, 0.0000],\n",
       "          [0.0335, 0.0530, 0.1001,  ..., 0.0470, 0.0158, 0.0405]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6108, 0.3892, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4331, 0.2113, 0.3556,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1216, 0.0606, 0.0381,  ..., 0.1237, 0.0000, 0.0000],\n",
       "          [0.0332, 0.0340, 0.0574,  ..., 0.0423, 0.0833, 0.0000],\n",
       "          [0.0802, 0.0447, 0.0673,  ..., 0.0987, 0.0522, 0.0492]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6437, 0.3563, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4380, 0.2777, 0.2842,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0593, 0.0465, 0.0458,  ..., 0.0772, 0.0000, 0.0000],\n",
       "          [0.0737, 0.0623, 0.0645,  ..., 0.0570, 0.0856, 0.0000],\n",
       "          [0.0427, 0.0589, 0.1317,  ..., 0.0457, 0.0616, 0.0496]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5312, 0.4688, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2530, 0.1644, 0.5826,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0289, 0.0810, 0.1110,  ..., 0.0446, 0.0000, 0.0000],\n",
       "          [0.0471, 0.1082, 0.0874,  ..., 0.1141, 0.0507, 0.0000],\n",
       "          [0.0892, 0.0349, 0.0590,  ..., 0.0870, 0.0632, 0.0470]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4568, 0.5432, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1619, 0.3192, 0.5189,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0953, 0.1058, 0.0869,  ..., 0.0786, 0.0000, 0.0000],\n",
       "          [0.1066, 0.0871, 0.0906,  ..., 0.0636, 0.0370, 0.0000],\n",
       "          [0.0766, 0.0570, 0.0838,  ..., 0.0478, 0.0544, 0.0448]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4627, 0.5373, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1702, 0.2972, 0.5326,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0587, 0.0343, 0.0914,  ..., 0.0692, 0.0000, 0.0000],\n",
       "          [0.0282, 0.0765, 0.0186,  ..., 0.0576, 0.0779, 0.0000],\n",
       "          [0.0176, 0.0609, 0.0349,  ..., 0.0501, 0.0704, 0.0428]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6896, 0.3104, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5136, 0.1682, 0.3183,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0658, 0.0535, 0.1000,  ..., 0.0787, 0.0000, 0.0000],\n",
       "          [0.0386, 0.0394, 0.0605,  ..., 0.1047, 0.0511, 0.0000],\n",
       "          [0.0768, 0.0128, 0.0298,  ..., 0.1691, 0.1140, 0.0605]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6156, 0.3844, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2436, 0.1747, 0.5817,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0472, 0.2348, 0.0708,  ..., 0.0437, 0.0000, 0.0000],\n",
       "          [0.0703, 0.0373, 0.0414,  ..., 0.0358, 0.1434, 0.0000],\n",
       "          [0.0363, 0.0914, 0.1392,  ..., 0.0283, 0.1852, 0.0276]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4059, 0.5941, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2557, 0.4917, 0.2526,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1163, 0.0969, 0.0437,  ..., 0.0579, 0.0000, 0.0000],\n",
       "          [0.0573, 0.1492, 0.0708,  ..., 0.0450, 0.0338, 0.0000],\n",
       "          [0.0586, 0.1040, 0.0879,  ..., 0.0246, 0.0703, 0.0310]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6981, 0.3019, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5396, 0.2529, 0.2075,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0483, 0.0551, 0.0850,  ..., 0.0441, 0.0000, 0.0000],\n",
       "          [0.0957, 0.0319, 0.0325,  ..., 0.1321, 0.0942, 0.0000],\n",
       "          [0.1020, 0.0611, 0.0884,  ..., 0.0562, 0.0313, 0.0508]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply softmax\n",
    "attention_score = F.softmax(output,dim=-1) # score transform equal to [0,1] \n",
    "attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "A =  attention_score @ V #  here not use transpose(-2,-1) may be attention_score has the token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4663, -0.0166, -0.4167,  ..., -0.0401, -0.1132,  0.3632],\n",
       "         [-0.1466,  0.2371, -0.2531,  ...,  0.0874, -0.1336,  0.1388],\n",
       "         [-0.0896,  0.0519, -0.0591,  ..., -0.0029, -0.0654, -0.3196],\n",
       "         ...,\n",
       "         [ 0.2636, -0.2210,  0.1402,  ..., -0.0442, -0.1109,  0.1679],\n",
       "         [ 0.0358,  0.1989, -0.0560,  ...,  0.0591, -0.2251, -0.0618],\n",
       "         [-0.4959,  0.5770, -0.3035,  ...,  0.1140, -0.3024,  0.1511]],\n",
       "\n",
       "        [[ 0.2800,  0.0649, -0.3240,  ...,  0.0558,  0.0565,  0.0300],\n",
       "         [-0.3127, -0.1461,  0.0296,  ..., -0.0818,  0.0963, -0.0012],\n",
       "         [ 0.0286,  0.1108, -0.2235,  ..., -0.0060, -0.0615, -0.2877],\n",
       "         ...,\n",
       "         [ 0.0754, -0.1843,  0.1082,  ...,  0.0102, -0.0855, -0.1104],\n",
       "         [ 0.1757,  0.1801, -0.1643,  ...,  0.0994, -0.3482,  0.0986],\n",
       "         [-0.4449,  0.1440,  0.1089,  ..., -0.0631,  0.1413,  0.0937]],\n",
       "\n",
       "        [[ 0.3564, -0.0333, -0.3294,  ...,  0.1741,  0.4237,  0.0613],\n",
       "         [-0.0877, -0.2983,  0.1739,  ..., -0.2151,  0.0735,  0.2495],\n",
       "         [-0.0231,  0.4562, -0.3300,  ...,  0.1914, -0.0117, -0.2903],\n",
       "         ...,\n",
       "         [ 0.0045, -0.2726,  0.0807,  ...,  0.0145, -0.3129,  0.2359],\n",
       "         [-0.0245,  0.0677,  0.0039,  ...,  0.0941, -0.2916, -0.0271],\n",
       "         [-0.4280,  0.2515, -0.0615,  ...,  0.1550,  0.2466,  0.0950]],\n",
       "\n",
       "        [[ 0.5370,  0.1608, -0.5051,  ...,  0.0978, -0.3064,  0.3099],\n",
       "         [ 0.0075, -0.1990,  0.1746,  ..., -0.1455, -0.5237, -0.0158],\n",
       "         [ 0.1363,  0.1137, -0.1612,  ...,  0.1515,  0.2121, -0.1852],\n",
       "         ...,\n",
       "         [ 0.2738, -0.2135, -0.0232,  ...,  0.0687, -0.0534, -0.0401],\n",
       "         [-0.2110,  0.4698, -0.1224,  ...,  0.2212, -0.1960, -0.1246],\n",
       "         [-0.4145,  0.3322, -0.0886,  ...,  0.0629,  0.1138,  0.2041]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate\n",
    "A = A.transpose(1,2).reshape(batch_size,-1,d_model)\n",
    "\n",
    "Wo = nn.Linear(d_model,d_model)\n",
    "output = Wo(A)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual connection\n",
    "output = output + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer normalization\n",
    "layer_norm = nn.LayerNorm(d_model)\n",
    "layer_norm_output = layer_norm(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed forward network TODO: idk\n",
    "output = nn.Linear(d_model,d_model * 4)(layer_norm_output)\n",
    "output = nn.ReLU()(output)\n",
    "output = nn.Linear(d_model * 4,d_model)(output)\n",
    "\n",
    "output = output + layer_norm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer normalization\n",
    "output = layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 100070])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final liner layer\n",
    "output = nn.Linear(d_model,max_token_value+1)(output)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6.1120e-06, 8.1475e-06, 2.9549e-06,  ..., 5.7906e-06,\n",
       "          6.7421e-06, 8.4356e-06],\n",
       "         [6.6594e-06, 6.1117e-06, 6.9420e-06,  ..., 1.8280e-05,\n",
       "          1.1392e-05, 1.3180e-05],\n",
       "         [3.7792e-06, 1.4917e-05, 7.2099e-06,  ..., 6.8160e-06,\n",
       "          6.6577e-06, 2.0404e-05],\n",
       "         ...,\n",
       "         [5.9275e-06, 6.6291e-06, 3.4847e-06,  ..., 9.1963e-06,\n",
       "          8.9139e-06, 1.6315e-05],\n",
       "         [4.5611e-06, 6.1672e-06, 5.8078e-06,  ..., 8.7649e-06,\n",
       "          1.1520e-05, 1.1059e-05],\n",
       "         [6.7396e-06, 8.3190e-06, 1.5697e-05,  ..., 6.8150e-06,\n",
       "          2.0611e-05, 1.6387e-05]],\n",
       "\n",
       "        [[4.0020e-06, 2.8324e-06, 8.4202e-06,  ..., 2.5911e-05,\n",
       "          1.1612e-05, 9.1395e-06],\n",
       "         [3.1889e-06, 9.1816e-06, 1.9994e-06,  ..., 7.9454e-06,\n",
       "          1.0197e-05, 1.9684e-05],\n",
       "         [8.3090e-06, 2.8057e-06, 4.4327e-06,  ..., 2.5911e-05,\n",
       "          4.4125e-06, 8.0696e-06],\n",
       "         ...,\n",
       "         [2.4293e-05, 5.5617e-06, 7.9825e-06,  ..., 6.1025e-06,\n",
       "          3.7104e-06, 3.9978e-06],\n",
       "         [8.8884e-06, 1.8942e-05, 1.3047e-05,  ..., 6.8555e-06,\n",
       "          3.9653e-06, 7.9860e-06],\n",
       "         [8.3998e-06, 6.2366e-06, 5.8510e-06,  ..., 5.0997e-06,\n",
       "          1.0500e-05, 2.7588e-06]],\n",
       "\n",
       "        [[9.7142e-06, 7.8862e-06, 1.4879e-05,  ..., 1.3387e-05,\n",
       "          1.6479e-05, 2.4159e-05],\n",
       "         [5.6102e-06, 3.9001e-06, 4.0380e-06,  ..., 1.6046e-05,\n",
       "          3.5066e-06, 3.2501e-06],\n",
       "         [1.6795e-05, 1.4212e-05, 4.6090e-06,  ..., 4.1950e-06,\n",
       "          1.1671e-05, 5.3503e-06],\n",
       "         ...,\n",
       "         [1.1290e-05, 5.5841e-06, 7.5490e-06,  ..., 4.2301e-06,\n",
       "          9.6074e-06, 1.7994e-05],\n",
       "         [8.1650e-06, 1.9722e-05, 1.1270e-05,  ..., 5.3399e-06,\n",
       "          3.4474e-06, 8.2822e-06],\n",
       "         [7.1314e-06, 7.7004e-06, 4.6932e-06,  ..., 1.2405e-05,\n",
       "          3.6731e-06, 8.5908e-06]],\n",
       "\n",
       "        [[6.6156e-06, 7.1460e-06, 5.0612e-06,  ..., 8.8676e-06,\n",
       "          1.1378e-05, 1.0518e-05],\n",
       "         [3.6624e-06, 6.3368e-06, 1.6514e-06,  ..., 6.6498e-06,\n",
       "          5.2376e-06, 1.2854e-05],\n",
       "         [3.1032e-06, 4.4655e-06, 8.5736e-06,  ..., 1.9842e-05,\n",
       "          3.6161e-06, 9.4368e-06],\n",
       "         ...,\n",
       "         [1.2292e-05, 1.6549e-05, 9.1974e-06,  ..., 3.4919e-06,\n",
       "          8.3934e-06, 5.6078e-06],\n",
       "         [3.9353e-06, 6.2097e-06, 2.9638e-05,  ..., 1.4577e-05,\n",
       "          8.6854e-06, 7.9612e-06],\n",
       "         [1.0558e-05, 3.2317e-06, 5.3889e-06,  ..., 7.2842e-06,\n",
       "          1.1507e-05, 5.3067e-06]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = F.softmax(output,dim=-1)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å…³'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(logits[0,0]))\n",
    "\n",
    "predicted_idx = torch.argmax(logits[0,0]).item() # get max score index\n",
    "\n",
    "encoding.decode([predicted_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
